{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501999da",
   "metadata": {},
   "source": [
    "### Problem 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Cleaned Text:\n",
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "5  @VirginAmerica seriously would pay $30 a fligh...   \n",
      "6  @VirginAmerica yes, nearly every time I fly VX...   \n",
      "7  @VirginAmerica Really missed a prime opportuni...   \n",
      "8    @virginamerica Well, I didn'tâ€¦but NOW I DO! :-D   \n",
      "9  @VirginAmerica it was amazing, and arrived an ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                                          what said  \n",
      "1  plus you have added commercial to the experien...  \n",
      "2  i did not today must mean i need to take anoth...  \n",
      "3  it is really aggressive to blast obnoxious ent...  \n",
      "4          and it is a really big bad thing about it  \n",
      "5  seriously would pay 30 a flight for seat that ...  \n",
      "6  yes nearly every time i fly vx this ear worm w...  \n",
      "7  really missed a prime opportunity for men with...  \n",
      "8                       well i did notbut now i do d  \n",
      "9  it wa amazing and arrived an hour early you ar...  \n",
      "\n",
      "Cleaned data saved to 'tweets_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df = pd.read_csv('tweets.csv')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Expand contractions \n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove emojis and special symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text, flags=re.UNICODE)\n",
    "    \n",
    "    # Remove punctuation (any remaining)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display results\n",
    "print(\"Original vs Cleaned Text:\")\n",
    "print(df[['text', 'cleaned_text']].head(10))\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('tweets_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned data saved to 'tweets_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e899ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Vocabulary size: 3000000\n",
      "Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained Google News Word2Vec model, you must have the model file downloaded\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Vocabulary size: {len(word2vec_model)}\")\n",
    "print(f\"Vector dimension: {word2vec_model.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447ad9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet vectors shape: (14640, 300)\n",
      "Sample vector (first 10 dimensions): [ 0.0652771  -0.025177    0.15722656 -0.00170898 -0.10888672  0.06860352\n",
      "  0.21191406 -0.1796875   0.07128906 -0.04376221]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tweet_to_vector(text, model, vector_size=300):\n",
    "    \"\"\"Convert a tweet to a fixed-length vector by averaging word vectors.\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    words = text.split()\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_vectors.append(model[word])\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert all tweets to vectors\n",
    "df['tweet_vector'] = df['cleaned_text'].apply(lambda x: tweet_to_vector(x, word2vec_model))\n",
    "\n",
    "# Create a matrix of all tweet vectors\n",
    "tweet_vectors = np.vstack(df['tweet_vector'].values)\n",
    "\n",
    "print(f\"Tweet vectors shape: {tweet_vectors.shape}\")\n",
    "print(f\"Sample vector (first 10 dimensions): {tweet_vectors[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column created:\n",
      "  airline_sentiment  target\n",
      "0           neutral       0\n",
      "1          positive       1\n",
      "2           neutral       0\n",
      "3          negative      -1\n",
      "4          negative      -1\n",
      "5          negative      -1\n",
      "6          positive       1\n",
      "7           neutral       0\n",
      "8          positive       1\n",
      "9          positive       1\n",
      "\n",
      "Target value counts:\n",
      "target\n",
      "-1    9178\n",
      " 0    3099\n",
      " 1    2363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create target column based on airline_sentiment, by mapping 'positive' to 1, 'negative' to -1, and 'neutral' to 0\n",
    "df['target'] = df['airline_sentiment'].map({'positive': 1, 'negative': -1, 'neutral': 0})\n",
    "\n",
    "print(\"Target column created:\")\n",
    "print(df[['airline_sentiment', 'target']].head(10))\n",
    "print(f\"\\nTarget value counts:\\n{df['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba718211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11712\n",
      "Testing set size: 2928\n",
      "\n",
      "Logistic Regression Accuracy on Test Set: 0.7876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare X and y\n",
    "X = tweet_vectors\n",
    "y = df['target'].values\n",
    "\n",
    "# Spliting the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Train Multiclass Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Calculate and report accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nLogistic Regression Accuracy on Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3571cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: It was a fantastic flight! The crew was so friendly and helpful.\n",
      "Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "def predict_tweet_sentiment(classifier, w2v_model, tweet):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a single tweet.\n",
    "    \n",
    "    Parameters:\n",
    "    - classifier: Trained classifier (e.g., LogisticRegression)\n",
    "    - w2v_model: Word2Vec model for vectorization\n",
    "    - tweet: String containing the tweet text\n",
    "    \n",
    "    Returns:\n",
    "    - String: 'positive', 'negative', or 'neutral'\n",
    "    \"\"\"\n",
    "    # Preprocess the tweet\n",
    "    cleaned_tweet = preprocess_text(tweet)\n",
    "    \n",
    "    # Convert to vector\n",
    "    tweet_vector = tweet_to_vector(cleaned_tweet, w2v_model)\n",
    "    \n",
    "    # Reshape for prediction (single sample)\n",
    "    tweet_vector = tweet_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = classifier.predict(tweet_vector)[0]\n",
    "    \n",
    "    # Map prediction to sentiment label\n",
    "    sentiment_map = {1: 'positive', -1: 'negative', 0: 'neutral'}\n",
    "    \n",
    "    return sentiment_map[prediction]\n",
    "\n",
    "# Test the function\n",
    "sample_tweet = \"It was a fantastic flight! The crew was so friendly and helpful.\"\n",
    "predicted_sentiment = predict_tweet_sentiment(lr_classifier, word2vec_model, sample_tweet)\n",
    "print(f\"Tweet: {sample_tweet}\")\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923518f",
   "metadata": {},
   "source": [
    "### Problem 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02f62621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99d6eda68574c72a3b1c00a7ab1d186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishan\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f93d3c35133442db7c7128c895c973a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f16e9652a247ba915fd63cbb5b7568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46326384997f4d2484b8991956929ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eb1b3d06d34cccaabd30e7ee72a523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7c91f05e5a422fa1a68aaa7614b27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76beea2d187a4d8aa11c42b7cacbf319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6908a5493fb4534a291f0eb757f9df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishan\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59aa63104f144aee9005172a61c5534f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c5411e96494af5aee15dfae2269221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f46fdc8d82e4133a69577346bae6734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca76c6d715a4715bd336edb231602af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9717f40932945b6a82eae04dd6c27b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87a03a28fcd41d9b89fc3487b8e707b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Tokenized dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample tokenized input (first 20 tokens): [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Load the IMDB dataset from Hugging Face\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(imdb_dataset)\n",
    "\n",
    "# Load the BERT tokenizer for bert-base-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_imdb = imdb_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenization complete!\")\n",
    "print(f\"Tokenized dataset: {tokenized_imdb}\")\n",
    "print(f\"\\nSample tokenized input (first 20 tokens): {tokenized_imdb['train'][0]['input_ids'][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf27dfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f93e449fdf4a40bfbb7155d2dac39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 1:49:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.312909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.248039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.330846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results: {'eval_loss': 0.24803878366947174, 'eval_runtime': 437.3916, 'eval_samples_per_second': 57.157, 'eval_steps_per_second': 7.145, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_imdb.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_imdb_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb['train'],\n",
    "    eval_dataset=tokenized_imdb['test'],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\nEvaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e8da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Performance on IMDB Test Set:\n",
      "Accuracy: 0.9336\n",
      "F1-Score: 0.9344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Get predictions on the test set\n",
    "predictions = trainer.predict(tokenized_imdb['test'])\n",
    "y_pred_bert = predictions.predictions.argmax(axis=-1)\n",
    "y_true_bert = predictions.label_ids\n",
    "\n",
    "# Calculate accuracy\n",
    "bert_accuracy = accuracy_score(y_true_bert, y_pred_bert)\n",
    "\n",
    "# Calculate F1-score (binary classification)\n",
    "bert_f1 = f1_score(y_true_bert, y_pred_bert)\n",
    "\n",
    "print(\"BERT Model Performance on IMDB Test Set:\")\n",
    "print(f\"Accuracy: {bert_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {bert_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e254c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: ./bert_imdb_finetuned\n",
      "Text: The movie was fantastic! I really loved it.\n",
      "Predicted label: positive (id=1)\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model + tokenizer after training\n",
    "save_dir = \"./bert_imdb_finetuned\"\n",
    "trainer.save_model(save_dir)          # saves model weights + config\n",
    "tokenizer.save_pretrained(save_dir)   # saves tokenizer files\n",
    "\n",
    "print(f\"Saved fine-tuned model to: {save_dir}\")\n",
    "\n",
    "# Load for inference on a sample text\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "inf_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "inf_model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inf_model.to(device)\n",
    "inf_model.eval()\n",
    "\n",
    "sample_text = \"The movie was fantastic! I really loved it.\"\n",
    "\n",
    "inputs = inf_tokenizer(\n",
    "    sample_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = inf_model(**inputs)\n",
    "    pred_id = outputs.logits.argmax(dim=-1).item()\n",
    "\n",
    "label_map = {0: \"negative\", 1: \"positive\"}  # IMDB convention\n",
    "print(\"Text:\", sample_text)\n",
    "print(\"Predicted label:\", label_map[pred_id], f\"(id={pred_id})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628094d2",
   "metadata": {},
   "source": [
    "## End-to-end Sentiment Analysis Pipeline (Problem 1 + Problem 2)\n",
    "\n",
    "This notebook implements two sentiment-analysis pipelines. In **Problem 1**, tweets are cleaned with `preprocess_text` (lowercasing, contraction expansion, removal of URLs/mentions/hashtags/punctuation, tokenization, and lemmatization). Each cleaned tweet is then converted into a fixed-length feature vector using `tweet_to_vector`, which averages **Google News Word2Vec** embeddings; this yields `tweet_vectors` / `X` with shape `(14640, 300)`. Labels are mapped to a 3-class target (`negative=-1`, `neutral=0`, `positive=1`) and a **multinomial Logistic Regression** model (`lr_classifier`) is trained on `X_train` and evaluated on `X_test` (accuracy stored in `accuracy`). In **Problem 2**, the **IMDB** dataset is tokenized with a BERT tokenizer and fine-tuned using `Trainer` with `BertForSequenceClassification`, producing strong test metrics (`bert_accuracy`, `bert_f1`).  \n",
    "\n",
    "The design contrasts a lightweight, interpretable baseline (Word2Vec + LR) with a higher-capacity contextual model (BERT). Key challenges include compute and memory: loading Word2Vec and fine-tuning BERT are resource-intensive, so GPU usage (`device`) and checkpointing/smaller batch sizes help. Preprocessing choices must also avoid removing sentiment-bearing cues; validating cleaning steps and monitoring OOV rates mitigates this.\n",
    "\n",
    "Note: couldn't upload bert_imdb_results and bert_imdb_finetuned folder due to file constraints of github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc156f00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
